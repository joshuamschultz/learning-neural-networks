{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X,W,b):\n",
    "    inputs = np.array(X)\n",
    "    weights = np.array(W)\n",
    "    bias = b\n",
    "    \n",
    "    prediction = inputs.dot(weights) + b\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    probabilities = []\n",
    "    for i in X:\n",
    "        sigmoid_function = 1 / (1 + np.exp(-i))\n",
    "        probabilities.append(sigmoid_function)\n",
    "        \n",
    "    return probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to have more than one point (2, -10) that we are going to test. Our goal is to see if we can improve the line we manually created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[2, -10], [2,10],[-5,-4],[-3,5]]\n",
    "W = [.38, 0.65]\n",
    "b = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.74, 10.26, -1.5 ,  5.11])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron(X, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get an array of predictons for each of the 4 points we entered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.060653903315652125,\n",
       " 0.9999649955375166,\n",
       " 0.18242552380635635,\n",
       " 0.9940001327725162]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability = sigmoid(perceptron(X, W, b))\n",
    "probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything below 50% in this example is red. So this is very red at only 6%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our points should be red, blue, red, blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want the model that is most correct. By this I mean, we want each prediction to highly probably, or highly improbable. If a model predicts an event it is correct about at 55%, versus another model for the same event (and being correct) at 90%, the 90% is most likely the better model. So we look at these probabilities for each point, and then see which is the best (most, highest probability predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we use the natural log of the probabilities so we can add them together rather than multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_like(x):\n",
    "    p = np.array(x)\n",
    "    ml = 0\n",
    "    \n",
    "    for i in p:\n",
    "        ml += -np.log(i)\n",
    "    return ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.510037509423813"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_like(probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiar to maximizing probability, when we take natural log the number gets smaller. So now we want to minimize what is called cross-entropy (sum of the negative natural logs of the sigmoid activations of the predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing to note is that is the errors need to be related to if the predictions are correct or not. Until now, we have assumed that the array of predictions was correct, that is $[0,1,0,1]$. What if it wasn't? In this case we would need to include that in our calculation of errors.\n",
    "\n",
    "We can do that by multiplying the classification of 0 or 1 with the natural log. \n",
    "\n",
    "$$ CE = - \\sum_{i=1}^{m} y_i\\ln(p_i) + (1-y_i)\\ln(1-p_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, if the prediction is correct (1) then the second term will 0 our and we get the - log of the probability. If the correct prediction is 0, the first term will 0 and we get the second term - giving us the still the - log of the probability on the other side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    \n",
    "    ce = -np.sum(Y * np.log(P) + (1-Y) * np.log(1-P))\n",
    "    \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7700375094238117"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([0,1,1,1],probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we usually view cross entropy as a type of average. So let's adjust our calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    m = len(Y)\n",
    "    \n",
    "    ce = -(1/m)*np.sum(Y * np.log(P) + (1-Y) * np.log(1-P))\n",
    "    \n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4425093773559529"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([0,1,1,1],probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note is that the probabilities are actually sigmoid functions of the predictions. So you can view the cross entropy as \n",
    "$$ CE = -\\frac{1}{m} \\sum_{i=1}^{m} y_i\\ln(\\sigma(Wx^{(i)}+b) + (1-y_i)\\ln(1-\\sigma(Wx^{(i)}+b)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can adjust weights in order to try to lower cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4524701784215939"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [.37, 0.68]\n",
    "probability = sigmoid(perceptron(X, W, b))\n",
    "cross_entropy([0,1,1,1],probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adjustment made the model worse with a higher cross entropy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
